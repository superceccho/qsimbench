"""
QSimBench
==================================
Functional client library for retrieving and sampling from the QSimBench dataset.
"""

import os
import logging
import threading
import json
import random
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse
import time
from dotenv import load_dotenv, set_key
from collections import deque

load_dotenv(override=True)

import requests
from requests.adapters import HTTPAdapter, Retry

# ---------------------------------------------------------------------------
# Custom exception for QSimBench errors
# ---------------------------------------------------------------------------
class QSimBenchError(Exception):
    """Base exception for QSimBench errors."""
    pass

# ---------------------------------------------------------------------------
# Logging configuration
# ---------------------------------------------------------------------------
logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

# ---------------------------------------------------------------------------
# HTTP client configuration
# ---------------------------------------------------------------------------
# Authentication header injection for GitHub API
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN") or None
AUTH_HEADERS = {}
if GITHUB_TOKEN:
    AUTH_HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"

# Retry strategy same as before
_RETRY_STRATEGY = Retry(
    total=3,
    backoff_factor=0.5,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods={"GET"},
)
_HTTP_ADAPTER = HTTPAdapter(max_retries=_RETRY_STRATEGY)

# Shared HTTP session with retries
_SESSION = requests.Session()
_SESSION.headers.update({"Accept": "application/vnd.github+json"})
_SESSION.headers.update(AUTH_HEADERS)
_SESSION.mount("https://", _HTTP_ADAPTER)
_SESSION.mount("http://", _HTTP_ADAPTER)

# ---------------------------------------------------------------------------
# Defaults & globals
# ---------------------------------------------------------------------------
DEFAULT_DATASET_URL = os.getenv(
    "QSIMBENCH_DATASET",
    "https://github.com/superceccho/qsimbench-dataset"
).rstrip("/")
DEFAULT_CACHE_DIR = Path(os.getenv(
    "QSIMBENCH_CACHE_DIR",
    Path(__file__).parent / ".qsimbench_cache"
))
DEFAULT_CACHE_TIMEOUT = int(os.getenv("QSIMBENCH_CACHE_TIMEOUT", 30 * 24 * 60 * 60))

if not os.path.exists(".env"):
    with open(".env", "w") as file:
        file.write("QSIMBENCH_DATASET=https://github.com/superceccho/qsimbench-dataset" \
                   f"QSIMBENCH_CACHE_TIMEOUT={30 * 24 * 60 * 60}")

# Ensure cache directory exists
DEFAULT_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Mutable configuration
DATASET_URL: str = DEFAULT_DATASET_URL
CACHE_DIR: Path = DEFAULT_CACHE_DIR
CACHE_TIMEOUT: int = DEFAULT_CACHE_TIMEOUT
RAW_URL = None
owner = None
repo = None

def get_raw_url() -> None:
    global owner, repo, RAW_URL

    url_parts = DATASET_URL.split("/")
    owner = url_parts[len(url_parts)-2]
    repo = url_parts[len(url_parts)-1]

    RAW_URL = f"https://raw.githubusercontent.com/{owner}/{repo}/refs/heads/main/dataset"

get_raw_url()

dataset_content = None

def get_dataset_content() -> None:
    main_url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/main"
    
    try:
        resp = _SESSION.get(main_url, timeout=1)
        resp.raise_for_status()
    except requests.HTTPError as e:
        raise QSimBenchError(f"GitHub API error: {e}") from e
    except requests.ConnectionError as e:
        raise QSimBenchError(f"Couldn't connect to the dataset: {e}")
    
    paths = resp.json()["tree"]
    for path in paths:
        if path["path"] == "dataset":
            dataset_url = path["url"]

    try:
        resp = _SESSION.get(dataset_url, timeout=1)
        resp.raise_for_status()
    except requests.HTTPError as e:
        raise QSimBenchError(f"GitHub API error: {e}") from e
    except requests.ConnectionError as e:
        raise QSimBenchError(f"Couldn't connect to the dataset: {e}")

    global dataset_content
    dataset_content = resp.json()["tree"]

get_dataset_content()

# Thread-safe cursor storage for sequential sampling
_CURSORS: Dict[Tuple[str, int, str, str], int] = {}
_CURSORS_LOCK = threading.RLock()

def load_versions():
    try:
        resp=_SESSION.get(f"{RAW_URL}/versions.json")
        resp.raise_for_status()
    except requests.HTTPError as e:
        raise QSimBenchError(f"HTTP error fetching {f"{RAW_URL}/versions.json"}: {e}") from e
    except requests.ConnectionError as e:
        raise QSimBenchError(f"Couldn't connect to the dataset: {e}")
    
    global versions_list, latest
    versions_list=json.loads(resp.text)
    if not versions_list:
        raise QSimBenchError("No versions avaible")
    latest=versions_list[len(versions_list)-1]

versions_list = []
latest = ""

load_versions()

# ---------------------------------------------------------------------------
# Configuration functions
# ---------------------------------------------------------------------------
        
def set_dataset_url(url: str, set_default=False) -> None:
    """
    Override the base dataset URL.

    Args:
        url: Must start with 'http://' or 'https://'.

    Raises:
        QSimBenchError: If URL is invalid.
    """
    global DATASET_URL
    if not url.startswith(("http://", "https://")):
        raise QSimBenchError("Dataset URL must start with 'http://' or 'https://'")
    DATASET_URL = url.rstrip("/")
    logger.debug(f"Dataset URL set to: {DATASET_URL}")

    get_raw_url()
    get_dataset_content()

    load_versions()

    if set_default:
        set_key(".env", "QSIMBENCH_DATASET", url)

def set_github_token(token: str, set_default=False) -> None:
    global GITHUB_TOKEN
    GITHUB_TOKEN = token
    _SESSION.headers.update({"Authorization": f"token {GITHUB_TOKEN}"})

    if set_default:
        set_key(".env", "GITHUB_TOKEN", token)

def set_cache_timeout(timeout: int, set_default=False) -> None:
    if not isinstance(timeout, int) or timeout < 0:
        raise QSimBenchError("Cache timout must be a positive integer")
    
    global CACHE_TIMEOUT
    CACHE_TIMEOUT = timeout

    if set_default:
        set_key(".env", "QSIMBENCH_CACHE_TIMEOUT", timeout)

# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------
def _multinomial_sample(
    agg: Dict[str, int],
    shots: int,
    seed: int
) -> Dict[str, int]:
    """
    Down-sample an aggregated distribution to exactly `shots` via multinomial sampling.

    Args:
        agg: Mapping from outcome bitstring to count.
        shots: Total draws desired.
        seed: Random seed.

    Returns:
        A new mapping with total counts == shots.

    Raises:
        QSimBenchError: If no counts are available.
    """
    total = sum(agg.values())
    if total <= 0:
        raise QSimBenchError("No counts available for multinomial sampling.")
    rng = random.Random(seed)
    bits = list(agg.keys())
    weights = [agg[b] / total for b in bits]
    sampled = rng.choices(bits, weights=weights, k=shots)
    result: Dict[str, int] = {}
    for b in sampled:
        result[b] = result.get(b, 0) + 1
    return result


def _download_and_cache(
    url: str,
    cache_path: Path,
    force: bool = False
) -> List[Dict[str, Any]]:
    """
    Download a JSONL file from `url` and cache it at `cache_path`.

    Args:
        url: Full URL to JSONL.
        cache_path: Local Path to cache.
        force: If True, ignore existing cache.

    Returns:
        List of JSON objects (one per line).

    Raises:
        QSimBenchError: On HTTP errors or empty result.
    """
    # Serve from cache if fresh
    if cache_path.exists() and not force:
        mtime = cache_path.stat().st_mtime
        now = time.time()
        if (now - mtime) < CACHE_TIMEOUT:
            logger.debug(f"Loading data from cache: {cache_path}")
            return [
                json.loads(line)
                for line in cache_path.read_text().splitlines()
                if line
            ]
        cache_path.unlink()

    logger.debug(f"Fetching data from URL: {url}")
    try:
        resp = _SESSION.get(url, timeout=1)
        resp.raise_for_status()
    except requests.HTTPError as e:
        if e.response.status_code != 404:
            raise QSimBenchError(f"HTTP error fetching {url}: {e}") from e
        else:
            return []
    except requests.ConnectionError as e:
        raise QSimBenchError(f"Couldn't connect to the dataset: {e}")

    records = [
        json.loads(line) for line in resp.text.splitlines() if line.strip()
    ]

    # Cache to disk
    cache_path.write_text("\n".join(json.dumps(r) for r in records))
    logger.debug(f"Cached {len(records)} records to {cache_path}")
    return records


def _get_data(
    algorithm: str,
    size: int,
    backend: str,
    version: str,
    circuit_kind: str = "circuit",
    force: bool = False
) -> List[Dict[str, Any]]:
    """
    Retrieve raw history records from the dataset.

    Args:
        algorithm: Algorithm name (non-empty).
        size: Positive integer problem size.
        backend: Backend identifier (non-empty).
        circuit_kind: Either "circuit" or "mirror".
        force: If True, bypass cache.

    Returns:
        List of record dicts.

    Raises:
        QSimBenchError: On invalid parameters or no records.
    """
    # Validate parameters
    if not algorithm or size <= 0 or not backend:
        raise QSimBenchError("algorithm, size, and backend must be valid.")
    kind = circuit_kind.lower()
    if kind not in {"circuit", "mirror"}:
        raise QSimBenchError("circuit_kind must be 'circuit' or 'mirror'.")

    # Build URL and cache path
    alg = algorithm.lower()
    be = backend.lower()
    file_name = f"{alg}_{size}_{be}.jsonl"
    url = f"{RAW_URL}/{version}/histories/{kind}/{file_name}"
    cache_path = CACHE_DIR / version / kind / file_name
    cache_path.parent.mkdir(parents=True, exist_ok=True)

    # Download & parse
    return _download_and_cache(url, cache_path, force=force)

# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------
def get_outcomes(
    algorithm: str,
    size: int,
    backend: str,
    shots: int = 1024,
    circuit_kind: str = "circuit",
    *,
    exact: bool = True,
    strategy: str = "sequential",
    versions: List[str] = [latest],
    seed: Optional[int] = None,
    force: bool = False
) -> Dict[str, int]:
    """
    Sample outcome counts for a given algorithm/size/backend.

    Args:
        algorithm: Algorithm name.
        size: Problem size > 0.
        backend: Backend name.
        shots: Total shots to sample (>0).
        circuit_kind: "circuit" or "mirror".
        exact: If True, enforce exactly `shots` via multinomial.
        strategy: "sequential" or "random".
        seed: Optional int seed for reproducibility.
        force: If True, refetch raw data ignoring cache.

    Returns:
        Mapping from outcome bitstring to count.

    Raises:
        QSimBenchError: On invalid args or sampling failures.
    """
    if not isinstance(shots, int) or shots <= 0:
        raise QSimBenchError("Parameter 'shots' must be > 0.")
    if strategy not in {"sequential", "random"}:
        raise QSimBenchError("Strategy must be 'sequential' or 'random'.")
    if not versions:
        raise QSimBenchError("At least on version") 
    
    data=[]
    for version in versions:
        new_data=_get_data(algorithm, size, backend, version, circuit_kind, force)
        data.extend(new_data)
    n = len(data)
    if n == 0:
        raise QSimBenchError("No records available to sample.")

    # Initialize RNGs
    master_rng = random.Random(seed)
    sample_seed = master_rng.randint(0, 2**32 - 1)
    exact_seed = master_rng.randint(0, 2**32 - 1)

    agg: Dict[str, int] = {}
    total = 0

    if strategy == "sequential":
        key = (algorithm, size, backend, circuit_kind)
        with _CURSORS_LOCK:
            start_idx = _CURSORS.get(key, 0)

        idx = start_idx
        consumed = 0
        while total < shots:
            if idx >= n:
                idx = 0
            rec = data[idx]
            s = int(rec.get("shots", 0))
            if s > 0:
                for bit, cnt in rec.get("data", {}).items():
                    agg[bit] = agg.get(bit, 0) + int(cnt)
                total += s
            idx += 1
            consumed += 1

        with _CURSORS_LOCK:
            _CURSORS[key] = idx % n

    else:  # random
        rng = random.Random(sample_seed)
        while total < shots:
            rec = rng.choice(data)
            s = int(rec.get("shots", 0))
            if s <= 0:
                continue
            for bit, cnt in rec.get("data", {}).items():
                agg[bit] = agg.get(bit, 0) + int(cnt)
            total += s

    # Exact down-sampling
    if exact and total > shots:
        agg = _multinomial_sample(agg, shots, exact_seed)

    return agg


@lru_cache()
def get_index(
    circuit_kind: str = "circuit",
    by_backend: bool = False,
    version: str = latest
) -> Dict[str, Any]:
    """
    List available algorithms, sizes, and backends in the dataset (via GitHub API).

    Args:
        circuit_kind: "circuit" or "mirror".
        by_backend: If True, invert mapping to backend→algorithm→sizes.

    Returns:
        Nested dict of available items.

    Raises:
        QSimBenchError: On URL parsing or HTTP errors.
    """
    kind = circuit_kind.lower()
    if kind not in {"circuit", "mirror"}:
        raise QSimBenchError("circuit_kind must be 'circuit' or 'mirror'.")
    
    if version not in versions_list:
        raise QSimBenchError(f"Version {version} doesn't exist")

    paths = dataset_content
    for path in paths:
        if path["path"] == version:
            version_url = path["url"]

    try:
        resp = _SESSION.get(version_url, timeout=1)
        resp.raise_for_status()
    except requests.HTTPError as e:
        raise QSimBenchError(f"GitHub API error: {e}") from e
    except requests.ConnectionError as e:
        raise QSimBenchError(f"Couldn't connect to the dataset: {e}")

    items = resp.json()["tree"]
    result: Dict[str, Any] = {}

    for item in items:
        name = item.get("path", "")
        if name in ["artifacts", "histories", "metadata.json"]:
            continue
        name = name.replace(".json", "")
        parts= name.split("_")
        parts = deque(parts)
        alg = parts.popleft()
        while True:
            part = parts.popleft()
            if part.isdigit():
                size = int(part)
                break
            alg += "_" + part

        parts.pop()

        backend = "_".join(parts)

        if not by_backend:
            result.setdefault(alg, {}).setdefault(size, []).append(backend)
        else:
            result.setdefault(backend, {}).setdefault(alg, []).append(size)

    return result


@lru_cache()
def get_metadata(
    algorithm: str,
    size: int,
    backend: str,
    version: str = latest
) -> List[Any]:
    """
    Fetch metadata JSON files for a given algorithm/size/backend/version.

    Args:
        algorithm: Algorithm name.
        size: Problem size.
        backend: Backend name.
        version: Dataset version

    Returns:
        List of parsed JSON metadata objects.

    Raises:
        QSimBenchError: On lookup or HTTP errors.
    """

    if version not in versions_list:
        raise QSimBenchError(f"Version {version} doesn't exist")

    paths = dataset_content
    for path in paths:
        if path["path"] == version:
            version_url = path["url"]
    
    try:
        resp = _SESSION.get(version_url, timeout=1)
        resp.raise_for_status()
    except requests.HTTPError as e:
        raise QSimBenchError(f"GitHub API error: {e}") from e
    except requests.ConnectionError as e:
        raise QSimBenchError(f"Couldn't connect to the dataset: {e}")

    files = [
        item["path"]
        for item in resp.json()["tree"]
        if item.get("path", "").startswith(f"{algorithm}_{size}_{backend}")
    ]
    if not files:
        raise QSimBenchError(
            f"No metadata files for {algorithm}_{size}_{backend} for version {version}"
        )

    metadata: List[Any] = []
    for fname in files:
        raw_url = f"{RAW_URL}/{version}/{fname}"
        try:
            r = _SESSION.get(raw_url, timeout=1)
            r.raise_for_status()
        except requests.HTTPError as e:
            raise QSimBenchError(f"Error fetching {raw_url}: {e}") from e
        except requests.ConnectionError as e:
            raise QSimBenchError(f"Couldn't connect to the dataset: {e}")
        try:
            metadata.append(r.json())  # try full parse
        except json.JSONDecodeError:
            for line in r.text.strip().splitlines():
                metadata.append(json.loads(line))

    return metadata

@lru_cache
def get_version_metadata(
    version: str = latest
) -> Dict[str, Any]:
    
    if version not in versions_list:
        raise QSimBenchError(f"Version {version} doesn't exist")
    
    try:
        resp=_SESSION.get(f"{RAW_URL}/{version}/metadata.json", timeout=1)
        resp.raise_for_status()
    except requests.HTTPError as e:
        raise QSimBenchError(f"Error fetching {f"{RAW_URL}/{version}/metadata.json"}: {e}") from e
    except requests.ConnectionError as e:
        raise QSimBenchError(f"Couldn't connect to the dataset: {e}")
    
    return json.loads(resp.text)

def get_versions() -> List[str]:
    return versions_list